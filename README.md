# WebRAG: AI-Powered Web Knowledge Retrieval & QA ğŸš€  

An AI-powered retrieval-augmented generation (RAG) system that extracts information from webpages, processes it using a vector database, and generates intelligent responses via a Large Language Model (LLM).  

## ğŸ“Œ Features  
- **ğŸ”— Web Scraping**: Extracts text from web pages using BeautifulSoup and requests.  
- **ğŸ“‚ Text Processing**: Cleans and chunks text for vector storage.  
- **ğŸ” Hybrid Retrieval**: Combines keyword-based (BM25) and dense retrieval (FAISS/Pinecone).  
- **ğŸ¤– LLM Integration**: Uses OpenAI/GPT API for intelligent response generation.  
- **âš¡ Fast API Service**: Deploys as an API with FastAPI for easy integration.  
- **ğŸ“¦ Dockerized Deployment**: Ready for cloud deployment (AWS/GCP).  

---
## ğŸ“‚ Project Structure  

```bash
WebRAG/
â”‚â”€â”€ data/                 # Store scraped web data (optional)
â”‚â”€â”€ embeddings/           # Store FAISS/Pinecone vectors
â”‚â”€â”€ models/               # Fine-tuned models (if applicable)
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py        # Web scraping module
â”‚   â”œâ”€â”€ preprocess.py     # Text cleaning and chunking
â”‚   â”œâ”€â”€ embedder.py       # Sentence embedding (e.g., MiniLM, Ada-002)
â”‚   â”œâ”€â”€ retriever.py      # Hybrid retrieval (BM25 + FAISS)
â”‚   â”œâ”€â”€ generator.py      # LLM-based response generation
â”‚   â”œâ”€â”€ api.py            # FastAPI service
â”‚â”€â”€ notebooks/            # Jupyter notebooks for testing
â”‚â”€â”€ docker/               # Docker deployment files
â”‚â”€â”€ requirements.txt      # Dependencies
â”‚â”€â”€ README.md             # Project Documentation
```

---
## **1ï¸âƒ£ Data Collection & Preprocessing**

Extract relevant text from a given **webpage URL** and preprocess it for efficient retrieval.  

### ğŸ”¹ Steps  
1. **Crawl Web Content**: Use `BeautifulSoup` and `requests` to extract text.  
2. **Text Cleaning**: Remove HTML tags, special characters, and stopwords (spaCy/NLTK).  
3. **Chunking**: Segment text into smaller units (e.g., 512 tokens per chunk).  

### ğŸ”¹ Run the script  
```bash
python src/scraper.py --url "https://example.com/article"
```

### âœ… Output Example
```
{
  "title": "Advancements in AI Healthcare",
  "content": [
    "Artificial intelligence is transforming healthcare by...",
    "One major application is medical image analysis...",
    "...more text extracted from the webpage..."
  ]
}
```

---
## 2ï¸âƒ£ Embedding & Vector Storage

ğŸ”¹ Goal

Convert extracted text into dense embeddings and store in FAISS/Pinecone for retrieval.

ğŸ”¹ Steps
	1.	Choose Embedding Model: Use SentenceTransformers or OpenAI text-embedding-ada-002.
	2.	Vectorize Chunks: Convert text chunks to numerical embeddings.
	3.	Store in FAISS/Pinecone: Create an index for fast retrieval.