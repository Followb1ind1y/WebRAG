import os
import pinecone
import getpass
import time
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers import Html2TextTransformer
from langchain_community.document_transformers import BeautifulSoupTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from pprint import pprint
from pinecone import Pinecone, ServerlessSpec
from uuid import uuid4


def init_pinecone(index_name="web-rag"):

    if not os.getenv("PINECONE_API_KEY"):
        os.environ["PINECONE_API_KEY"] = getpass.getpass("Enter your Pinecone API key: ")

    pinecone_api_key = os.environ.get("PINECONE_API_KEY")
    pc = Pinecone(api_key=pinecone_api_key)
    
    existing_indexes = [index_info["name"] for index_info in pc.list_indexes()]

    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»ºï¼ˆéœ€åŒ¹é…åµŒå…¥ç»´åº¦ï¼‰
    if index_name not in existing_indexes:
        pc.create_index(
            name=index_name,
            dimension=1536,  # OpenAI text-embedding-3-smallä¸º1536ç»´
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
        while not pc.describe_index(index_name).status["ready"]:
            time.sleep(1)
    
    index = pc.Index(index_name)

    return index

def fetch_webpage(urls):
    loader = AsyncHtmlLoader(urls)
    docs = loader.load()
    bs_transformer = BeautifulSoupTransformer()
    docs_transformed = bs_transformer.transform_documents(
    docs, unwanted_tags=["script", "style", "header", "footer", "nav", "aside"], tags_to_extract=["p"]
    )
    return docs_transformed

def split_text(text, chunk_size=512, chunk_overlap=50):
    """ä½¿ç”¨ LangChain è¿›è¡Œæ–‡æœ¬åˆ†å—"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    chunks = text_splitter.split_text(text)
    
    return chunks

# ä¸»æµç¨‹å‡½æ•°
def process_websites(urls: list, index_name: str = "web-rag"):
    # Step 1: æ‰¹é‡æŠ“å–ç½‘é¡µå¹¶æå–æ­£æ–‡
    print("å¼€å§‹æŠ“å–ç½‘é¡µ...")
    docs_transformed = fetch_webpage(urls)

    # Step 2: åˆå¹¶æ–‡æœ¬å¹¶åˆ†å—ï¼ˆä¼˜åŒ–åçš„æ‹†åˆ†é€»è¾‘ï¼‰
    print("æ–‡æœ¬åˆ†å—å¤„ç†...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1024, chunk_overlap=100
    )

    chunks_with_metadata = []
    for doc in docs_transformed:
        # æ‹†åˆ†æ–‡æœ¬å¹¶é™„åŠ å…ƒæ•°æ®
        chunks = text_splitter.split_text(doc.page_content)
        for chunk in chunks:
            chunks_with_metadata.append({
                "text": chunk,
                "metadata": {
                    "source": doc.metadata["source"],
                    "title": doc.metadata["title"],
                }
            })

    # Step 3: å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°Pinecone
    print("å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°Pinecone...")
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # å°†æ•°æ®è½¬æ¢ä¸ºDocumentæ ¼å¼
    documents = [
        Document(page_content=item["text"], metadata=item["metadata"])
        for item in chunks_with_metadata
    ]

    uuids = [str(uuid4()) for _ in range(len(documents))]

    # æ‰¹é‡å­˜å‚¨åˆ°Pinecone
    PineconeVectorStore.from_documents(
        documents=documents,
        embedding=embeddings,
        index_name=index_name,
        ids=uuids
    )
    print(f"âœ… æˆåŠŸå­˜å‚¨ {len(documents)} ä¸ªå—åˆ°ç´¢å¼• {index_name}")

if __name__ == "__main__":
    # åˆå§‹åŒ–Pinecone
    # index_name = init_pinecone()
    # print(f"ä½¿ç”¨ç´¢å¼• {index_name} å­˜å‚¨æ•°æ®")

    # # ğŸš€ å¼€å§‹çˆ¬å–
    # base_url = ["https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html",
    #             "https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html"]
    # process_websites(base_url)

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_store = PineconeVectorStore(index_name="web-rag", embedding=embeddings)
    results = vector_store.similarity_search(
    "Masked Language Modeling",
    k=2,
    # filter={"source": "tweet"},
)
for res in results:
    print(f"* {res.page_content} [{res.metadata}]")
